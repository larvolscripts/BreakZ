ğŸ“˜ Median PFS Prediction
 
This project predicts Median Progression-Free Survival (PFS) for oncology clinical trial arms using structured trial-armâ€“level datasets.
We combine tabular features, engineered oncology response metrics, disease & drug embeddings, and advanced ML models with strict Trial IDâ€“based leakage prevention.
 
 
---
 
ğŸ“Œ Project Overview
 
The goal is to build a robust, generalizable ML model that can estimate Median PFS using clinical trial metadata and response variables.
 
We explored:
 
LightGBM (GBDT + DART)
 
CatBoost (with Bayesian Optimization)
 
XGBoost
 
Stacking models
 
Disease & Drug text embeddings (BERT)
 
Feature engineering
 
Group-aware validation (Trial ID)
 
 
 
---
 
ğŸ“‚ Project Structure
 
MedianPFS/
â”‚
â”œâ”€â”€ data/                             # Cleaned datasets, merged files, embeddings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess/                   # Data cleaning, clustering, feature engineering
â”‚   â”œâ”€â”€ explore/                      # EDA, scatter plots, correlation analysis, PPT generation
â”‚   â”œâ”€â”€ new_features/
â”‚   â”‚   â”œâ”€â”€ product_decompose.py
â”‚   â”‚   â””â”€â”€ embed/
â”‚   â”‚       â”œâ”€â”€ disease_embed.py
â”‚   â”‚       â”œâ”€â”€ drug_embed.py
â”‚   â”‚       â”œâ”€â”€ merge_embed_DP.py
â”‚   â”‚       â”œâ”€â”€ train_embed_hybrid_77.py
â”‚   â”‚       â”œâ”€â”€ train_nopca.py
â”‚   â”‚       â”œâ”€â”€ train_cv.py
â”‚   â”‚       â””â”€â”€ tuning_lgbm_gbdt_optuna.py
â”‚   â”œâ”€â”€ train/                        # All ML training scripts
â”‚   â””â”€â”€ tuning/                       # Optuna, BO, DART tuning
â”‚
â”œâ”€â”€ outputs/                          # SHAP, CV results, pred-vs-actual, tuning outputs
â”œâ”€â”€ shap_summary.png
â”œâ”€â”€ shap_bar.png
â””â”€â”€ README.md
 
 
---
 
ğŸ§¹ 1. Data Cleaning & Preprocessing
 
Performed using scripts in src/preprocess/clean/.
 
âœ” Key steps:
 
Standardized column names
 
Removed invalid rows
 
Converted numeric features (ORR%, DoR, Arm N, PFS)
 
Removed Median PFS > 40 months as outliers
 
Filled missing values appropriately
 
 
 
---
 
ğŸ§¬ 2. Engineered Features
 
Feature engineering played a major role.
We generated:
 
Feature	Description
 
ORRÃ—DoR	ORR (%) Ã— Duration of Response
log(ORRÃ—DoR)	Log-normalized metric
Response_Count_Duration	Derived oncology response variable
Response_Percentage_Duration	% response Ã— duration
Product_Category	Immunotherapy / Targeted / Chemo / Hormonal / ADC / Other
MOA Cluster	KMeans grouping of mechanisms of action
Precise Area Cluster	KMeans grouping of disease areas
 
 
These improved interpretability and downstream modelling performance.
 
 
---
 
ğŸ§¬ 3. Disease & Drug Text Embeddings
 
We generated transformer-based 768-dim text embeddings for:
 
âœ” Disease Embeddings
 
From combined text:
 
Precise_Area_Name
 
Primary_MOA_all
 
Type
 
 
âœ” Drug Embeddings
 
Extracted active drug names â†’ embedded using the same biomedical BERT model.
 
Saved as:
 
disease_emb_0 ... disease_emb_767
 
drug_emb_0 ... drug_emb_767
 
 
âœ” Merged dataset
 
Merged by Trial ID + Arm ID â†’ Saved as:
 
MedianPFS_training_merged_with_embeddings.xlsx
 
 
---
 
âš™ï¸ 4. PCA Compression (Optional)
 
To reduce embedding dimensions:
 
768 â†’ 50 components
 
PCA used when needed for speed or regularization
 
Non-PCA version gave better performance, so we keep both.
 
 
 
---
 
ğŸ“Š 5. Exploratory Analysis
 
Scripts under src/explore/ generate:
 
ORR vs PFS plots
 
DoR vs PFS plots
 
ORRÃ—DoR vs PFS
 
log(ORRÃ—DoR) vs PFS
 
OS comparison plots
 
Automated PowerPoint report
 
Correlation heatmaps
 
 
These were packaged into:
 
Median_PFS_OS_Report.pptx
 
 
---
 
ğŸ¤– 6. Machine Learning Models
 
We trained multiple models:
 
âœ” LightGBM GBDT
 
Best baseline tabular model
â†’ RÂ² = 0.74
 
âœ” LightGBM + Disease/Drug Embeddings
 
Best single-split performance
â†’ RÂ² = 0.79
 
âœ” CatBoost (BO tuned)
 
â†’ RÂ² â‰ˆ 0.72
 
âœ” XGBoost, DART, Stacking
 
Underperformed relative to LightGBM
 
 
---
 
ğŸ” 7. Hyperparameter Tuning
 
âœ” Optuna (TPE)
 
Used extensively for:
 
LightGBM tuning
 
CatBoost tuning
 
PCA vs non-PCA selection
 
 
Best LightGBM parameters:
 
learning_rate=0.0037
n_estimators=1351
num_leaves=227
max_depth=6
min_child_samples=198
subsample=0.74
colsample_bytree=0.49
reg_alpha=1.09
reg_lambda=0.22
 
âœ” Bayesian Optimization for CatBoost
 
Best RÂ² â‰ˆ 0.72.
 
 
---
 
ğŸ§ª 8. Cross-Validation (Leakage-Free)
 
Used GroupKFold with Trial ID
 
â†’ Ensures trial arms from same trial never leak across folds.
 
Final CV results:
 
Model Type	Single-Split	5-Fold CV
 
Baseline (no embeddings)	0.74	0.68
With Embeddings (disease + drug)	0.79	0.55
 
 
Interpretation:
Embeddings help when training/validation distribution match (single-split)
But decrease generalization in strict Trial IDâ€“based CV.
 
 
---
 
ğŸ” 9. SHAP Explainability
 
Generated:
 
SHAP summary plot
 
SHAP bar importance
 
Feature ranking
 
Embedding component importance
 
 
Helps understand:
 
Which disease descriptors matter
 
Which drug properties matter
 
Which engineered features correlate with increase in PFS
 
 
 
---
 
ğŸ“¦ 10. Outputs
 
Everything saved under /outputs:
 
prediction_vs_actual.png
 
feature_importance.png
 
shap_summary.png
 
shap_bar.png
 
cv_summary.xlsx
 
Fold-wise prediction Excel files
 
Tuned parameter JSON
 
Final .pkl model files
 
 
 
---
 
ğŸš€ 11. Final Model Summary
 
âœ” Best single-split model (usable):
 
LightGBM + Disease Embeddings
â¡ RÂ² = 0.79, MAE â‰ˆ 2.05
 
âœ” Best generalizable model (recommended):
 
Baseline LightGBM without embeddings
â¡ 5-fold CV RÂ² = 0.68
 